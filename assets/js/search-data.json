{
  
    
        "post0": {
            "title": "Regression Analysis",
            "content": "End-to-end Machine Learning project . Welcome to Machine Learning Housing Corp.! Your task is to predict median house values in Californian districts, given a number of features from these districts. . This notebook contains all the sample code and solutions to the exercices in chapter 2. . Run in Google Colab | Setup . First, let&#39;s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20. . # Python ≥3.5 is required import sys assert sys.version_info &gt;= (3, 5) # Scikit-Learn ≥0.20 is required import sklearn assert sklearn.__version__ &gt;= &quot;0.20&quot; # Common imports import numpy as np import os # To plot pretty figures %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) # Where to save the figures PROJECT_ROOT_DIR = &quot;.&quot; CHAPTER_ID = &quot;end_to_end_project&quot; IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID) os.makedirs(IMAGES_PATH, exist_ok=True) def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300): path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension) print(&quot;Saving figure&quot;, fig_id) if tight_layout: plt.tight_layout() plt.savefig(path, format=fig_extension, dpi=resolution) # Ignore useless warnings (see SciPy issue #5998) import warnings warnings.filterwarnings(action=&quot;ignore&quot;, message=&quot;^internal gelsd&quot;) . Get the data . import os import tarfile import urllib DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot; HOUSING_PATH = os.path.join(&quot;datasets&quot;, &quot;housing&quot;) HOUSING_URL = DOWNLOAD_ROOT + &quot;datasets/housing/housing.tgz&quot; def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): if not os.path.isdir(housing_path): os.makedirs(housing_path) tgz_path = os.path.join(housing_path, &quot;housing.tgz&quot;) urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=housing_path) housing_tgz.close() . fetch_housing_data() . import pandas as pd def load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(housing_path, &quot;housing.csv&quot;) return pd.read_csv(csv_path) . housing = load_housing_data() housing.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | . housing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): longitude 20640 non-null float64 latitude 20640 non-null float64 housing_median_age 20640 non-null float64 total_rooms 20640 non-null float64 total_bedrooms 20433 non-null float64 population 20640 non-null float64 households 20640 non-null float64 median_income 20640 non-null float64 median_house_value 20640 non-null float64 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB . housing[&quot;ocean_proximity&quot;].value_counts() . &lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64 . housing.describe() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . count 20640.000000 | 20640.000000 | 20640.000000 | 20640.000000 | 20433.000000 | 20640.000000 | 20640.000000 | 20640.000000 | 20640.000000 | . mean -119.569704 | 35.631861 | 28.639486 | 2635.763081 | 537.870553 | 1425.476744 | 499.539680 | 3.870671 | 206855.816909 | . std 2.003532 | 2.135952 | 12.585558 | 2181.615252 | 421.385070 | 1132.462122 | 382.329753 | 1.899822 | 115395.615874 | . min -124.350000 | 32.540000 | 1.000000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | 0.499900 | 14999.000000 | . 25% -121.800000 | 33.930000 | 18.000000 | 1447.750000 | 296.000000 | 787.000000 | 280.000000 | 2.563400 | 119600.000000 | . 50% -118.490000 | 34.260000 | 29.000000 | 2127.000000 | 435.000000 | 1166.000000 | 409.000000 | 3.534800 | 179700.000000 | . 75% -118.010000 | 37.710000 | 37.000000 | 3148.000000 | 647.000000 | 1725.000000 | 605.000000 | 4.743250 | 264725.000000 | . max -114.310000 | 41.950000 | 52.000000 | 39320.000000 | 6445.000000 | 35682.000000 | 6082.000000 | 15.000100 | 500001.000000 | . %matplotlib inline import matplotlib.pyplot as plt housing.hist(bins=50, figsize=(20,15)) save_fig(&quot;attribute_histogram_plots&quot;) plt.show() . Saving figure attribute_histogram_plots . # to make this notebook&#39;s output identical at every run np.random.seed(42) . import numpy as np # For illustration only. Sklearn has train_test_split() def split_train_test(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices] . train_set, test_set = split_train_test(housing, 0.2) len(train_set) . 16512 . len(test_set) . 4128 . from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32 def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set] . The implementation of test_set_check() above works fine in both Python 2 and Python 3. In earlier releases, the following implementation was proposed, which supported any hash function, but was much slower and did not support Python 2: . import hashlib def test_set_check(identifier, test_ratio, hash=hashlib.md5): return hash(np.int64(identifier)).digest()[-1] &lt; 256 * test_ratio . If you want an implementation that supports any hash function and is compatible with both Python 2 and Python 3, here is one: . def test_set_check(identifier, test_ratio, hash=hashlib.md5): return bytearray(hash(np.int64(identifier)).digest())[-1] &lt; 256 * test_ratio . housing_with_id = housing.reset_index() # adds an `index` column train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;index&quot;) . housing_with_id[&quot;id&quot;] = housing[&quot;longitude&quot;] * 1000 + housing[&quot;latitude&quot;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, &quot;id&quot;) . test_set.head() . index longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity id . 8 8 | -122.26 | 37.84 | 42.0 | 2555.0 | 665.0 | 1206.0 | 595.0 | 2.0804 | 226700.0 | NEAR BAY | -122222.16 | . 10 10 | -122.26 | 37.85 | 52.0 | 2202.0 | 434.0 | 910.0 | 402.0 | 3.2031 | 281500.0 | NEAR BAY | -122222.15 | . 11 11 | -122.26 | 37.85 | 52.0 | 3503.0 | 752.0 | 1504.0 | 734.0 | 3.2705 | 241800.0 | NEAR BAY | -122222.15 | . 12 12 | -122.26 | 37.85 | 52.0 | 2491.0 | 474.0 | 1098.0 | 468.0 | 3.0750 | 213500.0 | NEAR BAY | -122222.15 | . 13 13 | -122.26 | 37.84 | 52.0 | 696.0 | 191.0 | 345.0 | 174.0 | 2.6736 | 191300.0 | NEAR BAY | -122222.16 | . from sklearn.model_selection import train_test_split train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) . test_set.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 20046 -119.01 | 36.06 | 25.0 | 1505.0 | NaN | 1392.0 | 359.0 | 1.6812 | 47700.0 | INLAND | . 3024 -119.46 | 35.14 | 30.0 | 2943.0 | NaN | 1565.0 | 584.0 | 2.5313 | 45800.0 | INLAND | . 15663 -122.44 | 37.80 | 52.0 | 3830.0 | NaN | 1310.0 | 963.0 | 3.4801 | 500001.0 | NEAR BAY | . 20484 -118.72 | 34.28 | 17.0 | 3051.0 | NaN | 1705.0 | 495.0 | 5.7376 | 218600.0 | &lt;1H OCEAN | . 9814 -121.93 | 36.62 | 34.0 | 2351.0 | NaN | 1063.0 | 428.0 | 3.7250 | 278000.0 | NEAR OCEAN | . housing[&quot;median_income&quot;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11a232400&gt; . housing[&quot;income_cat&quot;] = pd.cut(housing[&quot;median_income&quot;], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]) . housing[&quot;income_cat&quot;].value_counts() . 3 7236 2 6581 4 3639 5 2362 1 822 Name: income_cat, dtype: int64 . housing[&quot;income_cat&quot;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11a307c18&gt; . from sklearn.model_selection import StratifiedShuffleSplit split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing[&quot;income_cat&quot;]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] . strat_test_set[&quot;income_cat&quot;].value_counts() / len(strat_test_set) . 3 0.350533 2 0.318798 4 0.176357 5 0.114583 1 0.039729 Name: income_cat, dtype: float64 . housing[&quot;income_cat&quot;].value_counts() / len(housing) . 3 0.350581 2 0.318847 4 0.176308 5 0.114438 1 0.039826 Name: income_cat, dtype: float64 . def income_cat_proportions(data): return data[&quot;income_cat&quot;].value_counts() / len(data) train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) compare_props = pd.DataFrame({ &quot;Overall&quot;: income_cat_proportions(housing), &quot;Stratified&quot;: income_cat_proportions(strat_test_set), &quot;Random&quot;: income_cat_proportions(test_set), }).sort_index() compare_props[&quot;Rand. %error&quot;] = 100 * compare_props[&quot;Random&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props[&quot;Strat. %error&quot;] = 100 * compare_props[&quot;Stratified&quot;] / compare_props[&quot;Overall&quot;] - 100 . compare_props . Overall Stratified Random Rand. %error Strat. %error . 1 0.039826 | 0.039729 | 0.040213 | 0.973236 | -0.243309 | . 2 0.318847 | 0.318798 | 0.324370 | 1.732260 | -0.015195 | . 3 0.350581 | 0.350533 | 0.358527 | 2.266446 | -0.013820 | . 4 0.176308 | 0.176357 | 0.167393 | -5.056334 | 0.027480 | . 5 0.114438 | 0.114583 | 0.109496 | -4.318374 | 0.127011 | . for set_ in (strat_train_set, strat_test_set): set_.drop(&quot;income_cat&quot;, axis=1, inplace=True) . Discover and visualize the data to gain insights . housing = strat_train_set.copy() . housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;) save_fig(&quot;bad_visualization_plot&quot;) . Saving figure bad_visualization_plot . housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.1) save_fig(&quot;better_visualization_plot&quot;) . Saving figure better_visualization_plot . The argument sharex=False fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https://github.com/pandas-dev/pandas/issues/10611 ). Thanks to Wilmer Arellano for pointing it out. . housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, s=housing[&quot;population&quot;]/100, label=&quot;population&quot;, figsize=(10,7), c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True, sharex=False) plt.legend() save_fig(&quot;housing_prices_scatterplot&quot;) . Saving figure housing_prices_scatterplot . # Download the California image images_path = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, &quot;end_to_end_project&quot;) os.makedirs(images_path, exist_ok=True) DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/ageron/handson-ml2/master/&quot; filename = &quot;california.png&quot; print(&quot;Downloading&quot;, filename) url = DOWNLOAD_ROOT + &quot;images/end_to_end_project/&quot; + filename urllib.request.urlretrieve(url, os.path.join(images_path, filename)) . Downloading california.png . (&#39;./images/end_to_end_project/california.png&#39;, &lt;http.client.HTTPMessage at 0x7efdc6f79588&gt;) . import matplotlib.image as mpimg california_img=mpimg.imread(os.path.join(images_path, filename)) ax = housing.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, figsize=(10,7), s=housing[&#39;population&#39;]/100, label=&quot;Population&quot;, c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=False, alpha=0.4, ) plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5, cmap=plt.get_cmap(&quot;jet&quot;)) plt.ylabel(&quot;Latitude&quot;, fontsize=14) plt.xlabel(&quot;Longitude&quot;, fontsize=14) prices = housing[&quot;median_house_value&quot;] tick_values = np.linspace(prices.min(), prices.max(), 11) cbar = plt.colorbar(ticks=tick_values/prices.max()) cbar.ax.set_yticklabels([&quot;$%dk&quot;%(round(v/1000)) for v in tick_values], fontsize=14) cbar.set_label(&#39;Median House Value&#39;, fontsize=16) plt.legend(fontsize=16) save_fig(&quot;california_housing_prices_plot&quot;) plt.show() . Saving figure california_housing_prices_plot . corr_matrix = housing.corr() . corr_matrix[&quot;median_house_value&quot;].sort_values(ascending=False) . median_house_value 1.000000 median_income 0.687160 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population -0.026920 longitude -0.047432 latitude -0.142724 Name: median_house_value, dtype: float64 . # from pandas.tools.plotting import scatter_matrix # For older versions of Pandas from pandas.plotting import scatter_matrix attributes = [&quot;median_house_value&quot;, &quot;median_income&quot;, &quot;total_rooms&quot;, &quot;housing_median_age&quot;] scatter_matrix(housing[attributes], figsize=(12, 8)) save_fig(&quot;scatter_matrix_plot&quot;) . Saving figure scatter_matrix_plot . housing.plot(kind=&quot;scatter&quot;, x=&quot;median_income&quot;, y=&quot;median_house_value&quot;, alpha=0.1) plt.axis([0, 16, 0, 550000]) save_fig(&quot;income_vs_house_value_scatterplot&quot;) . Saving figure income_vs_house_value_scatterplot . housing[&quot;rooms_per_household&quot;] = housing[&quot;total_rooms&quot;]/housing[&quot;households&quot;] housing[&quot;bedrooms_per_room&quot;] = housing[&quot;total_bedrooms&quot;]/housing[&quot;total_rooms&quot;] housing[&quot;population_per_household&quot;]=housing[&quot;population&quot;]/housing[&quot;households&quot;] . corr_matrix = housing.corr() corr_matrix[&quot;median_house_value&quot;].sort_values(ascending=False) . median_house_value 1.000000 median_income 0.687160 rooms_per_household 0.146285 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population_per_household -0.021985 population -0.026920 longitude -0.047432 latitude -0.142724 bedrooms_per_room -0.259984 Name: median_house_value, dtype: float64 . housing.plot(kind=&quot;scatter&quot;, x=&quot;rooms_per_household&quot;, y=&quot;median_house_value&quot;, alpha=0.2) plt.axis([0, 5, 0, 520000]) plt.show() . housing.describe() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value rooms_per_household bedrooms_per_room population_per_household . count 16512.000000 | 16512.000000 | 16512.000000 | 16512.000000 | 16354.000000 | 16512.000000 | 16512.000000 | 16512.000000 | 16512.000000 | 16512.000000 | 16354.000000 | 16512.000000 | . mean -119.575834 | 35.639577 | 28.653101 | 2622.728319 | 534.973890 | 1419.790819 | 497.060380 | 3.875589 | 206990.920724 | 5.440341 | 0.212878 | 3.096437 | . std 2.001860 | 2.138058 | 12.574726 | 2138.458419 | 412.699041 | 1115.686241 | 375.720845 | 1.904950 | 115703.014830 | 2.611712 | 0.057379 | 11.584826 | . min -124.350000 | 32.540000 | 1.000000 | 6.000000 | 2.000000 | 3.000000 | 2.000000 | 0.499900 | 14999.000000 | 1.130435 | 0.100000 | 0.692308 | . 25% -121.800000 | 33.940000 | 18.000000 | 1443.000000 | 295.000000 | 784.000000 | 279.000000 | 2.566775 | 119800.000000 | 4.442040 | 0.175304 | 2.431287 | . 50% -118.510000 | 34.260000 | 29.000000 | 2119.500000 | 433.000000 | 1164.000000 | 408.000000 | 3.540900 | 179500.000000 | 5.232284 | 0.203031 | 2.817653 | . 75% -118.010000 | 37.720000 | 37.000000 | 3141.000000 | 644.000000 | 1719.250000 | 602.000000 | 4.744475 | 263900.000000 | 6.056361 | 0.239831 | 3.281420 | . max -114.310000 | 41.950000 | 52.000000 | 39320.000000 | 6210.000000 | 35682.000000 | 5358.000000 | 15.000100 | 500001.000000 | 141.909091 | 1.000000 | 1243.333333 | . Prepare the data for Machine Learning algorithms . housing = strat_train_set.drop(&quot;median_house_value&quot;, axis=1) # drop labels for training set housing_labels = strat_train_set[&quot;median_house_value&quot;].copy() . sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head() sample_incomplete_rows . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | NaN | 3296.0 | 1462.0 | 2.2708 | &lt;1H OCEAN | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | NaN | 3038.0 | 727.0 | 5.1762 | &lt;1H OCEAN | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | NaN | 999.0 | 386.0 | 4.6328 | &lt;1H OCEAN | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | NaN | 1039.0 | 391.0 | 1.6675 | INLAND | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | NaN | 3468.0 | 1405.0 | 3.1662 | &lt;1H OCEAN | . sample_incomplete_rows.dropna(subset=[&quot;total_bedrooms&quot;]) # option 1 . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . sample_incomplete_rows.drop(&quot;total_bedrooms&quot;, axis=1) # option 2 . longitude latitude housing_median_age total_rooms population households median_income ocean_proximity . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | 3296.0 | 1462.0 | 2.2708 | &lt;1H OCEAN | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | 3038.0 | 727.0 | 5.1762 | &lt;1H OCEAN | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | 999.0 | 386.0 | 4.6328 | &lt;1H OCEAN | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | 1039.0 | 391.0 | 1.6675 | INLAND | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | 3468.0 | 1405.0 | 3.1662 | &lt;1H OCEAN | . median = housing[&quot;total_bedrooms&quot;].median() sample_incomplete_rows[&quot;total_bedrooms&quot;].fillna(median, inplace=True) # option 3 . sample_incomplete_rows . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | 433.0 | 3296.0 | 1462.0 | 2.2708 | &lt;1H OCEAN | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | 433.0 | 3038.0 | 727.0 | 5.1762 | &lt;1H OCEAN | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | 433.0 | 999.0 | 386.0 | 4.6328 | &lt;1H OCEAN | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | 433.0 | 1039.0 | 391.0 | 1.6675 | INLAND | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | 433.0 | 3468.0 | 1405.0 | 3.1662 | &lt;1H OCEAN | . from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=&quot;median&quot;) . Remove the text attribute because median can only be calculated on numerical attributes: . housing_num = housing.drop(&quot;ocean_proximity&quot;, axis=1) # alternatively: housing_num = housing.select_dtypes(include=[np.number]) . imputer.fit(housing_num) . SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy=&#39;median&#39;, verbose=0) . imputer.statistics_ . array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409]) . Check that this is the same as manually computing the median of each attribute: . housing_num.median().values . array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409]) . Transform the training set: . X = imputer.transform(housing_num) . housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing.index) . housing_tr.loc[sample_incomplete_rows.index.values] . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income . 4629 -118.30 | 34.07 | 18.0 | 3759.0 | 433.0 | 3296.0 | 1462.0 | 2.2708 | . 6068 -117.86 | 34.01 | 16.0 | 4632.0 | 433.0 | 3038.0 | 727.0 | 5.1762 | . 17923 -121.97 | 37.35 | 30.0 | 1955.0 | 433.0 | 999.0 | 386.0 | 4.6328 | . 13656 -117.30 | 34.05 | 6.0 | 2155.0 | 433.0 | 1039.0 | 391.0 | 1.6675 | . 19252 -122.79 | 38.48 | 7.0 | 6837.0 | 433.0 | 3468.0 | 1405.0 | 3.1662 | . imputer.strategy . &#39;median&#39; . housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index) . housing_tr.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income . 17606 -121.89 | 37.29 | 38.0 | 1568.0 | 351.0 | 710.0 | 339.0 | 2.7042 | . 18632 -121.93 | 37.05 | 14.0 | 679.0 | 108.0 | 306.0 | 113.0 | 6.4214 | . 14650 -117.20 | 32.77 | 31.0 | 1952.0 | 471.0 | 936.0 | 462.0 | 2.8621 | . 3230 -119.61 | 36.31 | 25.0 | 1847.0 | 371.0 | 1460.0 | 353.0 | 1.8839 | . 3555 -118.59 | 34.23 | 17.0 | 6592.0 | 1525.0 | 4459.0 | 1463.0 | 3.0347 | . Now let&#39;s preprocess the categorical input feature, ocean_proximity: . housing_cat = housing[[&quot;ocean_proximity&quot;]] housing_cat.head(10) . ocean_proximity . 17606 &lt;1H OCEAN | . 18632 &lt;1H OCEAN | . 14650 NEAR OCEAN | . 3230 INLAND | . 3555 &lt;1H OCEAN | . 19480 INLAND | . 8879 &lt;1H OCEAN | . 13685 INLAND | . 4937 &lt;1H OCEAN | . 4861 &lt;1H OCEAN | . from sklearn.preprocessing import OrdinalEncoder ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) housing_cat_encoded[:10] . array([[0.], [0.], [4.], [1.], [0.], [1.], [0.], [1.], [0.], [0.]]) . ordinal_encoder.categories_ . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . from sklearn.preprocessing import OneHotEncoder cat_encoder = OneHotEncoder() housing_cat_1hot = cat_encoder.fit_transform(housing_cat) housing_cat_1hot . &lt;16512x5 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 16512 stored elements in Compressed Sparse Row format&gt; . By default, the OneHotEncoder class returns a sparse array, but we can convert it to a dense array if needed by calling the toarray() method: . housing_cat_1hot.toarray() . array([[1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 0., 1.], ..., [0., 1., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 1., 0.]]) . Alternatively, you can set sparse=False when creating the OneHotEncoder: . cat_encoder = OneHotEncoder(sparse=False) housing_cat_1hot = cat_encoder.fit_transform(housing_cat) housing_cat_1hot . array([[1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 0., 1.], ..., [0., 1., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 1., 0.]]) . cat_encoder.categories_ . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . Let&#39;s create a custom transformer to add extra attributes: . from sklearn.base import BaseEstimator, TransformerMixin # column index rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X): rooms_per_household = X[:, rooms_ix] / X[:, households_ix] population_per_household = X[:, population_ix] / X[:, households_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household] attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False) housing_extra_attribs = attr_adder.transform(housing.values) . housing_extra_attribs = pd.DataFrame( housing_extra_attribs, columns=list(housing.columns)+[&quot;rooms_per_household&quot;, &quot;population_per_household&quot;], index=housing.index) housing_extra_attribs.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity rooms_per_household population_per_household . 17606 -121.89 | 37.29 | 38 | 1568 | 351 | 710 | 339 | 2.7042 | &lt;1H OCEAN | 4.62537 | 2.0944 | . 18632 -121.93 | 37.05 | 14 | 679 | 108 | 306 | 113 | 6.4214 | &lt;1H OCEAN | 6.00885 | 2.70796 | . 14650 -117.2 | 32.77 | 31 | 1952 | 471 | 936 | 462 | 2.8621 | NEAR OCEAN | 4.22511 | 2.02597 | . 3230 -119.61 | 36.31 | 25 | 1847 | 371 | 1460 | 353 | 1.8839 | INLAND | 5.23229 | 4.13598 | . 3555 -118.59 | 34.23 | 17 | 6592 | 1525 | 4459 | 1463 | 3.0347 | &lt;1H OCEAN | 4.50581 | 3.04785 | . Now let&#39;s build a pipeline for preprocessing the numerical attributes: . from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler num_pipeline = Pipeline([ (&#39;imputer&#39;, SimpleImputer(strategy=&quot;median&quot;)), (&#39;attribs_adder&#39;, CombinedAttributesAdder()), (&#39;std_scaler&#39;, StandardScaler()), ]) housing_num_tr = num_pipeline.fit_transform(housing_num) . housing_num_tr . array([[-1.15604281, 0.77194962, 0.74333089, ..., -0.31205452, -0.08649871, 0.15531753], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0.21768338, -0.03353391, -0.83628902], [ 1.18684903, -1.34218285, 0.18664186, ..., -0.46531516, -0.09240499, 0.4222004 ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0.3469342 , -0.03055414, -0.52177644], [ 0.78221312, -0.85106801, 0.18664186, ..., 0.02499488, 0.06150916, -0.30340741], [-1.43579109, 0.99645926, 1.85670895, ..., -0.22852947, -0.09586294, 0.10180567]]) . from sklearn.compose import ColumnTransformer num_attribs = list(housing_num) cat_attribs = [&quot;ocean_proximity&quot;] full_pipeline = ColumnTransformer([ (&quot;num&quot;, num_pipeline, num_attribs), (&quot;cat&quot;, OneHotEncoder(), cat_attribs), ]) housing_prepared = full_pipeline.fit_transform(housing) . housing_prepared . array([[-1.15604281, 0.77194962, 0.74333089, ..., 0. , 0. , 0. ], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0. , 0. , 0. ], [ 1.18684903, -1.34218285, 0.18664186, ..., 0. , 0. , 1. ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0. , 0. , 0. ], [ 0.78221312, -0.85106801, 0.18664186, ..., 0. , 0. , 0. ], [-1.43579109, 0.99645926, 1.85670895, ..., 0. , 1. , 0. ]]) . housing_prepared.shape . (16512, 16) . For reference, here is the old solution based on a DataFrameSelector transformer (to just select a subset of the Pandas DataFrame columns), and a FeatureUnion: . from sklearn.base import BaseEstimator, TransformerMixin # Create a class to select numerical or categorical columns class OldDataFrameSelector(BaseEstimator, TransformerMixin): def __init__(self, attribute_names): self.attribute_names = attribute_names def fit(self, X, y=None): return self def transform(self, X): return X[self.attribute_names].values . Now let&#39;s join all these components into a big pipeline that will preprocess both the numerical and the categorical features: . num_attribs = list(housing_num) cat_attribs = [&quot;ocean_proximity&quot;] old_num_pipeline = Pipeline([ (&#39;selector&#39;, OldDataFrameSelector(num_attribs)), (&#39;imputer&#39;, SimpleImputer(strategy=&quot;median&quot;)), (&#39;attribs_adder&#39;, CombinedAttributesAdder()), (&#39;std_scaler&#39;, StandardScaler()), ]) old_cat_pipeline = Pipeline([ (&#39;selector&#39;, OldDataFrameSelector(cat_attribs)), (&#39;cat_encoder&#39;, OneHotEncoder(sparse=False)), ]) . from sklearn.pipeline import FeatureUnion old_full_pipeline = FeatureUnion(transformer_list=[ (&quot;num_pipeline&quot;, old_num_pipeline), (&quot;cat_pipeline&quot;, old_cat_pipeline), ]) . old_housing_prepared = old_full_pipeline.fit_transform(housing) old_housing_prepared . array([[-1.15604281, 0.77194962, 0.74333089, ..., 0. , 0. , 0. ], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0. , 0. , 0. ], [ 1.18684903, -1.34218285, 0.18664186, ..., 0. , 0. , 1. ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0. , 0. , 0. ], [ 0.78221312, -0.85106801, 0.18664186, ..., 0. , 0. , 0. ], [-1.43579109, 0.99645926, 1.85670895, ..., 0. , 1. , 0. ]]) . The result is the same as with the ColumnTransformer: . np.allclose(housing_prepared, old_housing_prepared) . True . Select and train a model . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . # let&#39;s try the full preprocessing pipeline on a few training instances some_data = housing.iloc[:5] some_labels = housing_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) print(&quot;Predictions:&quot;, lin_reg.predict(some_data_prepared)) . Predictions: [210644.60459286 317768.80697211 210956.43331178 59218.98886849 189747.55849879] . Compare against the actual values: . print(&quot;Labels:&quot;, list(some_labels)) . Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0] . some_data_prepared . array([[-1.15604281, 0.77194962, 0.74333089, -0.49323393, -0.44543821, -0.63621141, -0.42069842, -0.61493744, -0.31205452, -0.08649871, 0.15531753, 1. , 0. , 0. , 0. , 0. ], [-1.17602483, 0.6596948 , -1.1653172 , -0.90896655, -1.0369278 , -0.99833135, -1.02222705, 1.33645936, 0.21768338, -0.03353391, -0.83628902, 1. , 0. , 0. , 0. , 0. ], [ 1.18684903, -1.34218285, 0.18664186, -0.31365989, -0.15334458, -0.43363936, -0.0933178 , -0.5320456 , -0.46531516, -0.09240499, 0.4222004 , 0. , 0. , 0. , 0. , 1. ], [-0.01706767, 0.31357576, -0.29052016, -0.36276217, -0.39675594, 0.03604096, -0.38343559, -1.04556555, -0.07966124, 0.08973561, -0.19645314, 0. , 1. , 0. , 0. , 0. ], [ 0.49247384, -0.65929936, -0.92673619, 1.85619316, 2.41221109, 2.72415407, 2.57097492, -0.44143679, -0.35783383, -0.00419445, 0.2699277 , 1. , 0. , 0. , 0. , 0. ]]) . from sklearn.metrics import mean_squared_error housing_predictions = lin_reg.predict(housing_prepared) lin_mse = mean_squared_error(housing_labels, housing_predictions) lin_rmse = np.sqrt(lin_mse) lin_rmse . 68628.19819848922 . from sklearn.metrics import mean_absolute_error lin_mae = mean_absolute_error(housing_labels, housing_predictions) lin_mae . 49439.89599001897 . from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor(random_state=42) tree_reg.fit(housing_prepared, housing_labels) . DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=42, splitter=&#39;best&#39;) . housing_predictions = tree_reg.predict(housing_prepared) tree_mse = mean_squared_error(housing_labels, housing_predictions) tree_rmse = np.sqrt(tree_mse) tree_rmse . 0.0 . Fine-tune your model . from sklearn.model_selection import cross_val_score scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) tree_rmse_scores = np.sqrt(-scores) . def display_scores(scores): print(&quot;Scores:&quot;, scores) print(&quot;Mean:&quot;, scores.mean()) print(&quot;Standard deviation:&quot;, scores.std()) display_scores(tree_rmse_scores) . Scores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782 71115.88230639 75585.14172901 70262.86139133 70273.6325285 75366.87952553 71231.65726027] Mean: 71407.68766037929 Standard deviation: 2439.4345041191004 . lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) lin_rmse_scores = np.sqrt(-lin_scores) display_scores(lin_rmse_scores) . Scores: [66782.73843989 66960.118071 70347.95244419 74739.57052552 68031.13388938 71193.84183426 64969.63056405 68281.61137997 71552.91566558 67665.10082067] Mean: 69052.46136345083 Standard deviation: 2731.674001798348 . Note: we specify n_estimators=100 to be future-proof since the default value is going to change to 100 in Scikit-Learn 0.22 (for simplicity, this is not shown in the book). . from sklearn.ensemble import RandomForestRegressor forest_reg = RandomForestRegressor(n_estimators=100, random_state=42) forest_reg.fit(housing_prepared, housing_labels) . RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) . housing_predictions = forest_reg.predict(housing_prepared) forest_mse = mean_squared_error(housing_labels, housing_predictions) forest_rmse = np.sqrt(forest_mse) forest_rmse . 18603.515021376355 . from sklearn.model_selection import cross_val_score forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) forest_rmse_scores = np.sqrt(-forest_scores) display_scores(forest_rmse_scores) . Scores: [49519.80364233 47461.9115823 50029.02762854 52325.28068953 49308.39426421 53446.37892622 48634.8036574 47585.73832311 53490.10699751 50021.5852922 ] Mean: 50182.303100336096 Standard deviation: 2097.0810550985693 . scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) pd.Series(np.sqrt(-scores)).describe() . count 10.000000 mean 69052.461363 std 2879.437224 min 64969.630564 25% 67136.363758 50% 68156.372635 75% 70982.369487 max 74739.570526 dtype: float64 . from sklearn.svm import SVR svm_reg = SVR(kernel=&quot;linear&quot;) svm_reg.fit(housing_prepared, housing_labels) housing_predictions = svm_reg.predict(housing_prepared) svm_mse = mean_squared_error(housing_labels, housing_predictions) svm_rmse = np.sqrt(svm_mse) svm_rmse . 111094.6308539982 . from sklearn.model_selection import GridSearchCV param_grid = [ # try 12 (3×4) combinations of hyperparameters {&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, # then try 6 (2×3) combinations with bootstrap set as False {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}, ] forest_reg = RandomForestRegressor(random_state=42) # train across 5 folds, that&#39;s a total of (12+6)*5=90 rounds of training grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=&#39;neg_mean_squared_error&#39;, return_train_score=True) grid_search.fit(housing_prepared, housing_labels) . GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid=[{&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}], pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=True, scoring=&#39;neg_mean_squared_error&#39;, verbose=0) . The best hyperparameter combination found: . grid_search.best_params_ . {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} . grid_search.best_estimator_ . RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) . Let&#39;s look at the score of each hyperparameter combination tested during the grid search: . cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[&quot;mean_test_score&quot;], cvres[&quot;params&quot;]): print(np.sqrt(-mean_score), params) . 63669.05791727153 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 55627.16171305252 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 53384.57867637289 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} 60965.99185930139 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 52740.98248528835 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} 50377.344409590376 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} 58663.84733372485 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} 52006.15355973719 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} 50146.465964159885 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} 57869.25504027614 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} 51711.09443660957 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} 49682.25345942335 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} 62895.088889905004 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 54658.14484390074 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 59470.399594730654 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 3} 52725.01091081235 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 10} 57490.612956065226 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 51009.51445842374 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} . pd.DataFrame(grid_search.cv_results_) . mean_fit_time std_fit_time mean_score_time std_score_time param_max_features param_n_estimators param_bootstrap params split0_test_score split1_test_score ... mean_test_score std_test_score rank_test_score split0_train_score split1_train_score split2_train_score split3_train_score split4_train_score mean_train_score std_train_score . 0 0.060687 | 0.001166 | 0.004219 | 0.000192 | 2 | 3 | NaN | {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} | -3.837622e+09 | -4.147108e+09 | ... | -4.053749e+09 | 1.519609e+08 | 18 | -1.064113e+09 | -1.105142e+09 | -1.116550e+09 | -1.112342e+09 | -1.129650e+09 | -1.105559e+09 | 2.220402e+07 | . 1 0.197437 | 0.003169 | 0.011206 | 0.000903 | 2 | 10 | NaN | {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} | -3.047771e+09 | -3.254861e+09 | ... | -3.094381e+09 | 1.327046e+08 | 11 | -5.927175e+08 | -5.870952e+08 | -5.776964e+08 | -5.716332e+08 | -5.802501e+08 | -5.818785e+08 | 7.345821e+06 | . 2 0.595235 | 0.004583 | 0.032281 | 0.003131 | 2 | 30 | NaN | {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} | -2.689185e+09 | -3.021086e+09 | ... | -2.849913e+09 | 1.626879e+08 | 9 | -4.381089e+08 | -4.391272e+08 | -4.371702e+08 | -4.376955e+08 | -4.452654e+08 | -4.394734e+08 | 2.966320e+06 | . 3 0.099332 | 0.001025 | 0.003848 | 0.000278 | 4 | 3 | NaN | {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} | -3.730181e+09 | -3.786886e+09 | ... | -3.716852e+09 | 1.631421e+08 | 16 | -9.865163e+08 | -1.012565e+09 | -9.169425e+08 | -1.037400e+09 | -9.707739e+08 | -9.848396e+08 | 4.084607e+07 | . 4 0.327148 | 0.002355 | 0.011608 | 0.000659 | 4 | 10 | NaN | {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} | -2.666283e+09 | -2.784511e+09 | ... | -2.781611e+09 | 1.268562e+08 | 8 | -5.097115e+08 | -5.162820e+08 | -4.962893e+08 | -5.436192e+08 | -5.160297e+08 | -5.163863e+08 | 1.542862e+07 | . 5 0.972486 | 0.004962 | 0.030287 | 0.000912 | 4 | 30 | NaN | {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} | -2.387153e+09 | -2.588448e+09 | ... | -2.537877e+09 | 1.214603e+08 | 3 | -3.838835e+08 | -3.880268e+08 | -3.790867e+08 | -4.040957e+08 | -3.845520e+08 | -3.879289e+08 | 8.571233e+06 | . 6 0.133793 | 0.003362 | 0.003705 | 0.000122 | 6 | 3 | NaN | {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} | -3.119657e+09 | -3.586319e+09 | ... | -3.441447e+09 | 1.893141e+08 | 14 | -9.245343e+08 | -8.886939e+08 | -9.353135e+08 | -9.009801e+08 | -8.624664e+08 | -9.023976e+08 | 2.591445e+07 | . 7 0.446336 | 0.003465 | 0.012375 | 0.000601 | 6 | 10 | NaN | {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} | -2.549663e+09 | -2.782039e+09 | ... | -2.704640e+09 | 1.471542e+08 | 6 | -4.980344e+08 | -5.045869e+08 | -4.994664e+08 | -4.990325e+08 | -5.055542e+08 | -5.013349e+08 | 3.100456e+06 | . 8 1.355474 | 0.004652 | 0.032426 | 0.002536 | 6 | 30 | NaN | {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} | -2.370010e+09 | -2.583638e+09 | ... | -2.514668e+09 | 1.285063e+08 | 2 | -3.838538e+08 | -3.804711e+08 | -3.805218e+08 | -3.856095e+08 | -3.901917e+08 | -3.841296e+08 | 3.617057e+06 | . 9 0.171477 | 0.001233 | 0.003917 | 0.000316 | 8 | 3 | NaN | {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} | -3.353504e+09 | -3.348552e+09 | ... | -3.348851e+09 | 1.241864e+08 | 13 | -9.228123e+08 | -8.553031e+08 | -8.603321e+08 | -8.881964e+08 | -9.151287e+08 | -8.883545e+08 | 2.750227e+07 | . 10 0.577159 | 0.002378 | 0.011366 | 0.000736 | 8 | 10 | NaN | {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} | -2.571970e+09 | -2.718994e+09 | ... | -2.674037e+09 | 1.392720e+08 | 5 | -4.932416e+08 | -4.815238e+08 | -4.730979e+08 | -5.155367e+08 | -4.985555e+08 | -4.923911e+08 | 1.459294e+07 | . 11 1.739312 | 0.003247 | 0.030980 | 0.002178 | 8 | 30 | NaN | {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} | -2.357390e+09 | -2.546640e+09 | ... | -2.468326e+09 | 1.091647e+08 | 1 | -3.841658e+08 | -3.744500e+08 | -3.773239e+08 | -3.882250e+08 | -3.810005e+08 | -3.810330e+08 | 4.871017e+06 | . 12 0.094185 | 0.001679 | 0.004966 | 0.000238 | 2 | 3 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_est... | -3.785816e+09 | -4.166012e+09 | ... | -3.955792e+09 | 1.900966e+08 | 17 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 13 0.313036 | 0.002217 | 0.013152 | 0.001291 | 2 | 10 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_est... | -2.810721e+09 | -3.107789e+09 | ... | -2.987513e+09 | 1.539231e+08 | 10 | -6.056477e-02 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -2.967449e+00 | -6.056027e-01 | 1.181156e+00 | . 14 0.125452 | 0.002900 | 0.004471 | 0.000239 | 3 | 3 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_est... | -3.618324e+09 | -3.441527e+09 | ... | -3.536728e+09 | 7.795196e+07 | 15 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -6.072840e+01 | -1.214568e+01 | 2.429136e+01 | . 15 0.413205 | 0.003939 | 0.013399 | 0.001358 | 3 | 10 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_est... | -2.757999e+09 | -2.851737e+09 | ... | -2.779927e+09 | 6.286611e+07 | 7 | -2.089484e+01 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -5.465556e+00 | -5.272080e+00 | 8.093117e+00 | . 16 0.155506 | 0.002193 | 0.004849 | 0.000425 | 4 | 3 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_est... | -3.134040e+09 | -3.559375e+09 | ... | -3.305171e+09 | 1.879203e+08 | 12 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 17 0.516612 | 0.001973 | 0.013149 | 0.000834 | 4 | 10 | False | {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_est... | -2.525578e+09 | -2.710011e+09 | ... | -2.601971e+09 | 1.088031e+08 | 4 | -0.000000e+00 | -1.514119e-02 | -0.000000e+00 | -0.000000e+00 | -0.000000e+00 | -3.028238e-03 | 6.056477e-03 | . 18 rows × 23 columns . from sklearn.model_selection import RandomizedSearchCV from scipy.stats import randint param_distribs = { &#39;n_estimators&#39;: randint(low=1, high=200), &#39;max_features&#39;: randint(low=1, high=8), } forest_reg = RandomForestRegressor(random_state=42) rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs, n_iter=10, cv=5, scoring=&#39;neg_mean_squared_error&#39;, random_state=42) rnd_search.fit(housing_prepared, housing_labels) . RandomizedSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_iter=10, n_jobs=None, param_distributions={&#39;n_estimators&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x127470860&gt;, &#39;max_features&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x127470828&gt;}, pre_dispatch=&#39;2*n_jobs&#39;, random_state=42, refit=True, return_train_score=&#39;warn&#39;, scoring=&#39;neg_mean_squared_error&#39;, verbose=0) . cvres = rnd_search.cv_results_ for mean_score, params in zip(cvres[&quot;mean_test_score&quot;], cvres[&quot;params&quot;]): print(np.sqrt(-mean_score), params) . 49150.657232934034 {&#39;max_features&#39;: 7, &#39;n_estimators&#39;: 180} 51389.85295710133 {&#39;max_features&#39;: 5, &#39;n_estimators&#39;: 15} 50796.12045980556 {&#39;max_features&#39;: 3, &#39;n_estimators&#39;: 72} 50835.09932039744 {&#39;max_features&#39;: 5, &#39;n_estimators&#39;: 21} 49280.90117886215 {&#39;max_features&#39;: 7, &#39;n_estimators&#39;: 122} 50774.86679035961 {&#39;max_features&#39;: 3, &#39;n_estimators&#39;: 75} 50682.75001237282 {&#39;max_features&#39;: 3, &#39;n_estimators&#39;: 88} 49608.94061293652 {&#39;max_features&#39;: 5, &#39;n_estimators&#39;: 100} 50473.57642831875 {&#39;max_features&#39;: 3, &#39;n_estimators&#39;: 150} 64429.763804893395 {&#39;max_features&#39;: 5, &#39;n_estimators&#39;: 2} . feature_importances = grid_search.best_estimator_.feature_importances_ feature_importances . array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02, 1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01, 5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02, 1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03]) . extra_attribs = [&quot;rooms_per_hhold&quot;, &quot;pop_per_hhold&quot;, &quot;bedrooms_per_room&quot;] #cat_encoder = cat_pipeline.named_steps[&quot;cat_encoder&quot;] # old solution cat_encoder = full_pipeline.named_transformers_[&quot;cat&quot;] cat_one_hot_attribs = list(cat_encoder.categories_[0]) attributes = num_attribs + extra_attribs + cat_one_hot_attribs sorted(zip(feature_importances, attributes), reverse=True) . [(0.3661589806181342, &#39;median_income&#39;), (0.1647809935615905, &#39;INLAND&#39;), (0.10879295677551573, &#39;pop_per_hhold&#39;), (0.07334423551601242, &#39;longitude&#39;), (0.0629090704826203, &#39;latitude&#39;), (0.05641917918195401, &#39;rooms_per_hhold&#39;), (0.05335107734767581, &#39;bedrooms_per_room&#39;), (0.041143798478729635, &#39;housing_median_age&#39;), (0.014874280890402767, &#39;population&#39;), (0.014672685420543237, &#39;total_rooms&#39;), (0.014257599323407807, &#39;households&#39;), (0.014106483453584102, &#39;total_bedrooms&#39;), (0.010311488326303787, &#39;&lt;1H OCEAN&#39;), (0.002856474637320158, &#39;NEAR OCEAN&#39;), (0.00196041559947807, &#39;NEAR BAY&#39;), (6.028038672736599e-05, &#39;ISLAND&#39;)] . final_model = grid_search.best_estimator_ X_test = strat_test_set.drop(&quot;median_house_value&quot;, axis=1) y_test = strat_test_set[&quot;median_house_value&quot;].copy() X_test_prepared = full_pipeline.transform(X_test) final_predictions = final_model.predict(X_test_prepared) final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse) . final_rmse . 47730.22690385927 . We can compute a 95% confidence interval for the test RMSE: . from scipy import stats confidence = 0.95 squared_errors = (final_predictions - y_test) ** 2 np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors))) . array([45685.10470776, 49691.25001878]) . We could compute the interval manually like this: . m = len(squared_errors) mean = squared_errors.mean() tscore = stats.t.ppf((1 + confidence) / 2, df=m - 1) tmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m) np.sqrt(mean - tmargin), np.sqrt(mean + tmargin) . (45685.10470776014, 49691.25001877871) . Alternatively, we could use a z-scores rather than t-scores: . zscore = stats.norm.ppf((1 + confidence) / 2) zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m) np.sqrt(mean - zmargin), np.sqrt(mean + zmargin) . (45685.717918136594, 49690.68623889426) . Extra material . A full pipeline with both preparation and prediction . full_pipeline_with_predictor = Pipeline([ (&quot;preparation&quot;, full_pipeline), (&quot;linear&quot;, LinearRegression()) ]) full_pipeline_with_predictor.fit(housing, housing_labels) full_pipeline_with_predictor.predict(some_data) . array([210644.60459286, 317768.80697211, 210956.43331178, 59218.98886849, 189747.55849879]) . Model persistence using joblib . my_model = full_pipeline_with_predictor . import joblib joblib.dump(my_model, &quot;my_model.pkl&quot;) # DIFF #... my_model_loaded = joblib.load(&quot;my_model.pkl&quot;) # DIFF . Example SciPy distributions for RandomizedSearchCV . from scipy.stats import geom, expon geom_distrib=geom(0.5).rvs(10000, random_state=42) expon_distrib=expon(scale=1).rvs(10000, random_state=42) plt.hist(geom_distrib, bins=50) plt.show() plt.hist(expon_distrib, bins=50) plt.show() . Exercise solutions . 1. . Question: Try a Support Vector Machine regressor (sklearn.svm.SVR), with various hyperparameters such as kernel=&quot;linear&quot; (with various values for the C hyperparameter) or kernel=&quot;rbf&quot; (with various values for the C and gamma hyperparameters). Don&#39;t worry about what these hyperparameters mean for now. How does the best SVR predictor perform? . from sklearn.model_selection import GridSearchCV param_grid = [ {&#39;kernel&#39;: [&#39;linear&#39;], &#39;C&#39;: [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]}, {&#39;kernel&#39;: [&#39;rbf&#39;], &#39;C&#39;: [1.0, 3.0, 10., 30., 100., 300., 1000.0], &#39;gamma&#39;: [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]}, ] svm_reg = SVR() grid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring=&#39;neg_mean_squared_error&#39;, verbose=2) grid_search.fit(housing_prepared, housing_labels) . Fitting 5 folds for each of 50 candidates, totalling 250 fits [CV] C=10.0, kernel=linear ........................................... . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. . [CV] ............................ C=10.0, kernel=linear, total= 5.1s [CV] C=10.0, kernel=linear ........................................... . [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 7.3s remaining: 0.0s . [CV] ............................ C=10.0, kernel=linear, total= 5.1s [CV] C=10.0, kernel=linear ........................................... [CV] ............................ C=10.0, kernel=linear, total= 5.2s [CV] C=10.0, kernel=linear ........................................... [CV] ............................ C=10.0, kernel=linear, total= 5.1s [CV] C=10.0, kernel=linear ........................................... [CV] ............................ C=10.0, kernel=linear, total= 5.1s [CV] C=30.0, kernel=linear ........................................... [CV] ............................ C=30.0, kernel=linear, total= 5.0s [CV] C=30.0, kernel=linear ........................................... [CV] ............................ C=30.0, kernel=linear, total= 5.0s [CV] C=30.0, kernel=linear ........................................... [CV] ............................ C=30.0, kernel=linear, total= 5.3s [CV] C=30.0, kernel=linear ........................................... [CV] ............................ C=30.0, kernel=linear, total= 5.1s [CV] C=30.0, kernel=linear ........................................... [CV] ............................ C=30.0, kernel=linear, total= 5.0s [CV] C=100.0, kernel=linear .......................................... [CV] ........................... C=100.0, kernel=linear, total= 5.1s [CV] C=100.0, kernel=linear .......................................... [CV] ........................... C=100.0, kernel=linear, total= 5.0s [CV] C=100.0, kernel=linear .......................................... [CV] ........................... C=100.0, kernel=linear, total= 5.1s [CV] C=100.0, kernel=linear .......................................... [CV] ........................... C=100.0, kernel=linear, total= 5.0s [CV] C=100.0, kernel=linear .......................................... [CV] ........................... C=100.0, kernel=linear, total= 5.0s [CV] C=300.0, kernel=linear .......................................... [CV] ........................... C=300.0, kernel=linear, total= 5.1s &lt;&lt;434 more lines&gt;&gt; [CV] C=1000.0, gamma=0.1, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.1, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=0.1, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.1, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=0.3, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=0.3, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=0.3, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=0.3, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=0.3, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=0.3, kernel=rbf, total= 8.0s [CV] C=1000.0, gamma=1.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total= 8.6s [CV] C=1000.0, gamma=1.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total= 8.7s [CV] C=1000.0, gamma=1.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total= 9.4s [CV] C=1000.0, gamma=1.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total= 9.1s [CV] C=1000.0, gamma=1.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=1.0, kernel=rbf, total= 8.9s [CV] C=1000.0, gamma=3.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total= 10.9s [CV] C=1000.0, gamma=3.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total= 11.1s [CV] C=1000.0, gamma=3.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total= 11.1s [CV] C=1000.0, gamma=3.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total= 11.0s [CV] C=1000.0, gamma=3.0, kernel=rbf ................................. [CV] .................. C=1000.0, gamma=3.0, kernel=rbf, total= 11.1s . [Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed: 52.8min finished . GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;auto_deprecated&#39;, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid=[{&#39;kernel&#39;: [&#39;linear&#39;], &#39;C&#39;: [10.0, 30.0, 100.0, 300.0, 1000.0, 3000.0, 10000.0, 30000.0]}, {&#39;kernel&#39;: [&#39;rbf&#39;], &#39;C&#39;: [1.0, 3.0, 10.0, 30.0, 100.0, 300.0, 1000.0], &#39;gamma&#39;: [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]}], pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=&#39;neg_mean_squared_error&#39;, verbose=2) . The best model achieves the following score (evaluated using 5-fold cross validation): . negative_mse = grid_search.best_score_ rmse = np.sqrt(-negative_mse) rmse . 70363.90313964167 . That&#39;s much worse than the RandomForestRegressor. Let&#39;s check the best hyperparameters found: . grid_search.best_params_ . {&#39;C&#39;: 30000.0, &#39;kernel&#39;: &#39;linear&#39;} . The linear kernel seems better than the RBF kernel. Notice that the value of C is the maximum tested value. When this happens you definitely want to launch the grid search again with higher values for C (removing the smallest values), because it is likely that higher values of C will be better. . 2. . Question: Try replacing GridSearchCV with RandomizedSearchCV. . from sklearn.model_selection import RandomizedSearchCV from scipy.stats import expon, reciprocal # see https://docs.scipy.org/doc/scipy/reference/stats.html # for `expon()` and `reciprocal()` documentation and more probability distribution functions. # Note: gamma is ignored when kernel is &quot;linear&quot; param_distribs = { &#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;], &#39;C&#39;: reciprocal(20, 200000), &#39;gamma&#39;: expon(scale=1.0), } svm_reg = SVR() rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs, n_iter=50, cv=5, scoring=&#39;neg_mean_squared_error&#39;, verbose=2, random_state=42) rnd_search.fit(housing_prepared, housing_labels) . Fitting 5 folds for each of 50 candidates, totalling 250 fits [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ...... . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. . [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear, total= 5.6s [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ...... . [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 8.0s remaining: 0.0s . [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear, total= 5.7s [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ...... [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear, total= 5.8s [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ...... [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear, total= 5.4s [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear ...... [CV] C=629.782329591372, gamma=3.010121430917521, kernel=linear, total= 5.8s [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ...... [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total= 10.8s [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ...... [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total= 11.2s [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ...... [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total= 10.8s [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ...... [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total= 11.4s [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf ...... [CV] C=26290.206464300216, gamma=0.9084469696321253, kernel=rbf, total= 11.7s [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf ..... [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total= 9.0s [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf ..... [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total= 9.3s [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf ..... [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total= 9.0s [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf ..... [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total= 9.3s [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf ..... [CV] C=84.14107900575871, gamma=0.059838768608680676, kernel=rbf, total= 8.9s [CV] C=432.37884813148855, gamma=0.15416196746656105, kernel=linear .. [CV] C=432.37884813148855, gamma=0.15416196746656105, kernel=linear, total= 5.2s &lt;&lt;434 more lines&gt;&gt; [CV] C=61217.04421344494, gamma=1.6279689407405564, kernel=rbf ....... [CV] C=61217.04421344494, gamma=1.6279689407405564, kernel=rbf, total= 36.9s [CV] C=61217.04421344494, gamma=1.6279689407405564, kernel=rbf ....... [CV] C=61217.04421344494, gamma=1.6279689407405564, kernel=rbf, total= 34.8s [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........ [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total= 9.3s [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........ [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total= 9.2s [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........ [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total= 9.4s [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........ [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total= 9.5s [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf ........ [CV] C=926.9787684096649, gamma=2.147979593060577, kernel=rbf, total= 9.2s [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ...... [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total= 13.4s [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ...... [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total= 13.5s [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ...... [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total= 12.2s [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ...... [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total= 13.6s [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear ...... [CV] C=33946.157064934, gamma=2.2642426492862313, kernel=linear, total= 12.7s [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear .... [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total= 34.2s [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear .... [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total= 24.6s [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear .... [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total= 38.2s [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear .... [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total= 27.7s [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear .... [CV] C=84789.82947739525, gamma=0.3176359085304841, kernel=linear, total= 21.0s . [Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed: 70.9min finished . RandomizedSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;auto_deprecated&#39;, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False), fit_params=None, iid=&#39;warn&#39;, n_iter=50, n_jobs=None, param_distributions={&#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;], &#39;C&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x1411fbbe0&gt;, &#39;gamma&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x1411fb780&gt;}, pre_dispatch=&#39;2*n_jobs&#39;, random_state=42, refit=True, return_train_score=&#39;warn&#39;, scoring=&#39;neg_mean_squared_error&#39;, verbose=2) . The best model achieves the following score (evaluated using 5-fold cross validation): . negative_mse = rnd_search.best_score_ rmse = np.sqrt(-negative_mse) rmse . 54767.99053704408 . Now this is much closer to the performance of the RandomForestRegressor (but not quite there yet). Let&#39;s check the best hyperparameters found: . rnd_search.best_params_ . {&#39;C&#39;: 157055.10989448498, &#39;gamma&#39;: 0.26497040005002437, &#39;kernel&#39;: &#39;rbf&#39;} . This time the search found a good set of hyperparameters for the RBF kernel. Randomized search tends to find better hyperparameters than grid search in the same amount of time. . Let&#39;s look at the exponential distribution we used, with scale=1.0. Note that some samples are much larger or smaller than 1.0, but when you look at the log of the distribution, you can see that most values are actually concentrated roughly in the range of exp(-2) to exp(+2), which is about 0.1 to 7.4. . expon_distrib = expon(scale=1.) samples = expon_distrib.rvs(10000, random_state=42) plt.figure(figsize=(10, 4)) plt.subplot(121) plt.title(&quot;Exponential distribution (scale=1.0)&quot;) plt.hist(samples, bins=50) plt.subplot(122) plt.title(&quot;Log of this distribution&quot;) plt.hist(np.log(samples), bins=50) plt.show() . The distribution we used for C looks quite different: the scale of the samples is picked from a uniform distribution within a given range, which is why the right graph, which represents the log of the samples, looks roughly constant. This distribution is useful when you don&#39;t have a clue of what the target scale is: . reciprocal_distrib = reciprocal(20, 200000) samples = reciprocal_distrib.rvs(10000, random_state=42) plt.figure(figsize=(10, 4)) plt.subplot(121) plt.title(&quot;Reciprocal distribution (scale=1.0)&quot;) plt.hist(samples, bins=50) plt.subplot(122) plt.title(&quot;Log of this distribution&quot;) plt.hist(np.log(samples), bins=50) plt.show() . The reciprocal distribution is useful when you have no idea what the scale of the hyperparameter should be (indeed, as you can see on the figure on the right, all scales are equally likely, within the given range), whereas the exponential distribution is best when you know (more or less) what the scale of the hyperparameter should be. . 3. . Question: Try adding a transformer in the preparation pipeline to select only the most important attributes. . from sklearn.base import BaseEstimator, TransformerMixin def indices_of_top_k(arr, k): return np.sort(np.argpartition(np.array(arr), -k)[-k:]) class TopFeatureSelector(BaseEstimator, TransformerMixin): def __init__(self, feature_importances, k): self.feature_importances = feature_importances self.k = k def fit(self, X, y=None): self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k) return self def transform(self, X): return X[:, self.feature_indices_] . Note: this feature selector assumes that you have already computed the feature importances somehow (for example using a RandomForestRegressor). You may be tempted to compute them directly in the TopFeatureSelector&#39;s fit() method, however this would likely slow down grid/randomized search since the feature importances would have to be computed for every hyperparameter combination (unless you implement some sort of cache). . Let&#39;s define the number of top features we want to keep: . k = 5 . Now let&#39;s look for the indices of the top k features: . top_k_feature_indices = indices_of_top_k(feature_importances, k) top_k_feature_indices . array([ 0, 1, 7, 9, 12]) . np.array(attributes)[top_k_feature_indices] . array([&#39;longitude&#39;, &#39;latitude&#39;, &#39;median_income&#39;, &#39;pop_per_hhold&#39;, &#39;INLAND&#39;], dtype=&#39;&lt;U18&#39;) . Let&#39;s double check that these are indeed the top k features: . sorted(zip(feature_importances, attributes), reverse=True)[:k] . [(0.3661589806181342, &#39;median_income&#39;), (0.1647809935615905, &#39;INLAND&#39;), (0.10879295677551573, &#39;pop_per_hhold&#39;), (0.07334423551601242, &#39;longitude&#39;), (0.0629090704826203, &#39;latitude&#39;)] . Looking good... Now let&#39;s create a new pipeline that runs the previously defined preparation pipeline, and adds top k feature selection: . preparation_and_feature_selection_pipeline = Pipeline([ (&#39;preparation&#39;, full_pipeline), (&#39;feature_selection&#39;, TopFeatureSelector(feature_importances, k)) ]) . housing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing) . Let&#39;s look at the features of the first 3 instances: . housing_prepared_top_k_features[0:3] . array([[-1.15604281, 0.77194962, -0.61493744, -0.08649871, 0. ], [-1.17602483, 0.6596948 , 1.33645936, -0.03353391, 0. ], [ 1.18684903, -1.34218285, -0.5320456 , -0.09240499, 0. ]]) . Now let&#39;s double check that these are indeed the top k features: . housing_prepared[0:3, top_k_feature_indices] . array([[-1.15604281, 0.77194962, -0.61493744, -0.08649871, 0. ], [-1.17602483, 0.6596948 , 1.33645936, -0.03353391, 0. ], [ 1.18684903, -1.34218285, -0.5320456 , -0.09240499, 0. ]]) . Works great! :) . 4. . Question: Try creating a single pipeline that does the full data preparation plus the final prediction. . prepare_select_and_predict_pipeline = Pipeline([ (&#39;preparation&#39;, full_pipeline), (&#39;feature_selection&#39;, TopFeatureSelector(feature_importances, k)), (&#39;svm_reg&#39;, SVR(**rnd_search.best_params_)) ]) . prepare_select_and_predict_pipeline.fit(housing, housing_labels) . Pipeline(memory=None, steps=[(&#39;preparation&#39;, ColumnTransformer(n_jobs=None, remainder=&#39;drop&#39;, sparse_threshold=0.3, transformer_weights=None, transformers=[(&#39;num&#39;, Pipeline(memory=None, steps=[(&#39;imputer&#39;, SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy=&#39;median&#39;, verbos... gamma=0.26497040005002437, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False))]) . Let&#39;s try the full pipeline on a few instances: . some_data = housing.iloc[:4] some_labels = housing_labels.iloc[:4] print(&quot;Predictions: t&quot;, prepare_select_and_predict_pipeline.predict(some_data)) print(&quot;Labels: t t&quot;, list(some_labels)) . Predictions: [203214.28978849 371846.88152572 173295.65441612 47328.3970888 ] Labels: [286600.0, 340600.0, 196900.0, 46300.0] . Well, the full pipeline seems to work fine. Of course, the predictions are not fantastic: they would be better if we used the best RandomForestRegressor that we found earlier, rather than the best SVR. . 5. . Question: Automatically explore some preparation options using GridSearchCV. . param_grid = [{ &#39;preparation__num__imputer__strategy&#39;: [&#39;mean&#39;, &#39;median&#39;, &#39;most_frequent&#39;], &#39;feature_selection__k&#39;: list(range(1, len(feature_importances) + 1)) }] grid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5, scoring=&#39;neg_mean_squared_error&#39;, verbose=2) grid_search_prep.fit(housing, housing_labels) . Fitting 5 folds for each of 48 candidates, totalling 240 fits [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean . [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. . [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean . [Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 8.9s remaining: 0.0s . [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean, total= 6.2s [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean [CV] feature_selection__k=1, preparation__num__imputer__strategy=mean, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=median [CV] feature_selection__k=1, preparation__num__imputer__strategy=median, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=median [CV] feature_selection__k=1, preparation__num__imputer__strategy=median, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=median [CV] feature_selection__k=1, preparation__num__imputer__strategy=median, total= 6.2s [CV] feature_selection__k=1, preparation__num__imputer__strategy=median [CV] feature_selection__k=1, preparation__num__imputer__strategy=median, total= 6.2s [CV] feature_selection__k=1, preparation__num__imputer__strategy=median [CV] feature_selection__k=1, preparation__num__imputer__strategy=median, total= 6.2s [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total= 6.1s [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total= 6.3s [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total= 6.2s [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total= 6.2s [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=1, preparation__num__imputer__strategy=most_frequent, total= 6.2s [CV] feature_selection__k=2, preparation__num__imputer__strategy=mean [CV] feature_selection__k=2, preparation__num__imputer__strategy=mean, total= 6.5s &lt;&lt;414 more lines&gt;&gt; [CV] feature_selection__k=15, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=15, preparation__num__imputer__strategy=most_frequent, total= 21.7s [CV] feature_selection__k=15, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=15, preparation__num__imputer__strategy=most_frequent, total= 26.9s [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean, total= 25.4s [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean, total= 26.4s [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean, total= 24.8s [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean, total= 25.9s [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean [CV] feature_selection__k=16, preparation__num__imputer__strategy=mean, total= 21.8s [CV] feature_selection__k=16, preparation__num__imputer__strategy=median [CV] feature_selection__k=16, preparation__num__imputer__strategy=median, total= 22.8s [CV] feature_selection__k=16, preparation__num__imputer__strategy=median [CV] feature_selection__k=16, preparation__num__imputer__strategy=median, total= 26.1s [CV] feature_selection__k=16, preparation__num__imputer__strategy=median [CV] feature_selection__k=16, preparation__num__imputer__strategy=median, total= 24.5s [CV] feature_selection__k=16, preparation__num__imputer__strategy=median [CV] feature_selection__k=16, preparation__num__imputer__strategy=median, total= 20.1s [CV] feature_selection__k=16, preparation__num__imputer__strategy=median [CV] feature_selection__k=16, preparation__num__imputer__strategy=median, total= 25.0s [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total= 22.3s [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total= 26.0s [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total= 23.3s [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total= 23.9s [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent [CV] feature_selection__k=16, preparation__num__imputer__strategy=most_frequent, total= 26.5s . [Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed: 74.2min finished . GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=Pipeline(memory=None, steps=[(&#39;preparation&#39;, ColumnTransformer(n_jobs=None, remainder=&#39;drop&#39;, sparse_threshold=0.3, transformer_weights=None, transformers=[(&#39;num&#39;, Pipeline(memory=None, steps=[(&#39;imputer&#39;, SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy=&#39;median&#39;, verbos... gamma=0.26497040005002437, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False))]), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid=[{&#39;preparation__num__imputer__strategy&#39;: [&#39;mean&#39;, &#39;median&#39;, &#39;most_frequent&#39;], &#39;feature_selection__k&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}], pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=&#39;neg_mean_squared_error&#39;, verbose=2) . grid_search_prep.best_params_ . {&#39;feature_selection__k&#39;: 15, &#39;preparation__num__imputer__strategy&#39;: &#39;most_frequent&#39;} . The best imputer strategy is most_frequent and apparently almost all features are useful (15 out of 16). The last one (ISLAND) seems to just add some noise. . Congratulations! You already know quite a lot about Machine Learning. :) .",
            "url": "https://ishmaelasabere.github.io/blog/2020/08/23/Regression-Analysis.html",
            "relUrl": "/2020/08/23/Regression-Analysis.html",
            "date": " • Aug 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ishmaelasabere.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ishmaelasabere.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ishmaelasabere.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ishmaelasabere.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}